<!-- omit in toc -->
# Awesome Large Language and Foundation Models in Agriculture: A Comprehensive Review

[![Awesome](https://awesome.re/badge.svg)](https://github.com/JiajiaLi04/Agriculture-Foundation-Models
) ![](https://img.shields.io/github/stars/JiajiaLi04/Agriculture-Foundation-Models?style=social)

![](https://img.shields.io/github/last-commit/JiajiaLi04/Agriculture-Foundation-Models?color=#00FA9A) ![](https://img.shields.io/badge/PaperNumber-39-blue) ![](https://img.shields.io/badge/PRs-Welcome-red) 

A curated list of awesome **Foundation Models in Agriculture** papers üî•üî•üî•. 

Currently maintained by <ins>[Jiajia Li](xx) @ MSU</ins>. 



**<font color='red'>Work still in progress</font>**  üöÄ, **we appreciate any suggestions and contributions** ‚ù§Ô∏è.

---

<!-- What is instruction learning?
Why instruction learning?
-->

<!-- TODO
## Our scope:
We aim to stay up-to-date with the most innovative developments in the field and gain valuable insights into the future of instruction-learning technology.üëÄ organize a systematic and comprehensive overview of instructional learning.

1. Stay up-to-date with the most innovative developments in this field.
2. Gain valuable insights into the future of instruction-learning technology.
3. 
-->


<!-- omit in toc -->
## How to contribute?

If you have any suggestions or find any missed papers, feel free to reach out or submit a [pull request](https://github.com/JiajiaLi04/Agriculture-Foundation-Models/pulls):

1. Use following markdown format.

```markdown
*Author 1, Author 2, and Author 3.* **Paper Title.**  <ins>Conference/Journal/Preprint</ins> Year. [[pdf](link)]; [[other resources](link)].
```
<!-- >1. **Paper Title.** *Author 1, Author 2, and Author 3.* Conference/Journal/Preprint Year. [[pdf](link)]. -->

2. If one preprint paper has multiple versions, please use **the earliest submitted year**.
   
3. Display the papers in a **year descending order** (the latest, the first).


<!-- omit in toc -->
## Citation

Find this repository helpful? üòä  

Please consider citing our paper. üëáüëáüëá

*(**Note that the current version of our survey is only a draft, and we are still working on it.**)* üöÄ

```
@article{li2024foundation,
  title={Foundation models in smart agriculture: Basics, opportunities, and challenges},
  author={Li, Jiajia and Xu, Mingle and Xiang, Lirong and Chen, Dong and Zhuang, Weichao and Yin, Xunyuan and Li, Zhaojian},
  journal={Computers and Electronics in Agriculture},
  volume={222},
  pages={109032},
  year={2024},
  publisher={Elsevier}
}
```

## New Papers
- Wu, Jing, Zhixin Lai, Suiyao Chen, Ran Tao, Pan Zhao, and Naira Hovakimyan. "The New Agronomists: Language Models are Experts in Crop Management." arXiv preprint arXiv:2403.19839 (2024).

<!-- omit in toc -->
## üîç Table of Contents 

- [1. üíÅüèΩ‚Äç‚ôÄÔ∏è Introduction](#1-Ô∏è-introduction)
- [2. üéì Surveys and Tutorials](#2--surveys-and-tutorials)
- [3. üóÇÔ∏è Taxonomy](#3-Ô∏è-taxonomy)
  - [3.1 Language foundation models](#31-language-foundation-models)
  - [3.2 Vision foundation models](#32-vision-foundation-models)
  - [3.3 Multimodal foundation models](#33-multimodal-foundation-models)
  - [3.4 Reinforcement learning foundation models](#34-reinforcement-learning-foundation-models)
- [4. ü§ñ Applications](#4--applications)


## 1. üíÅüèΩ‚Äç‚ôÄÔ∏è Introduction
Why foundation models instead of traditional deep learning models?
- üëâ **Pre-trained Knowledge.** By training on vast and diverse datasets, FMs possess a form of "general intelligence" that encompasses knowledge of the world, language, vision, and their specific training domains.
- üëâ **Fine-tuning Flexibility.** FMs demonstrate superior performance to be fine-tuned for particular tasks or datasets, saving the computational and temporal investments required to train extensive models from scratch.
- üëâ **Data Efficiency.** FMs harness their foundational knowledge, exhibiting remarkable performance even in the face of limited task-specific data, which is effective for scenarios with data scarcity issues. 

## 2. üéì Surveys and Tutorials 
1. Moor, Michael, et al. "Foundation models for generalist medical artificial intelligence." Nature 616.7956 (2023): 259-265. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Foundation+models+for+generalist+medical+artificial+intelligence&btnG=) [[Paper]](https://www.nature.com/articles/s41586-023-05881-4)
2. Mai, Gengchen, et al. "On the opportunities and challenges of foundation models for geospatial artificial intelligence." arXiv preprint arXiv:2304.06798 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=On+the+Opportunities+and+Challenges+of+Foundation+Models+for+Geospatial+Artificial+Intelligence&btnG=) [[Paper]](https://arxiv.org/abs/2304.06798)
3. Stella, Francesco, Cosimo Della Santina, and Josie Hughes. "How can LLMs transform the robotic design process?." Nature Machine Intelligence (2023): 1-4.  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=How+can+LLMs+transform+the+robotic+design+process&btnG=) [[Paper]](https://www.nature.com/articles/s42256-023-00669-7)
4. Zhang, Chaoning, et al. "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering." arXiv preprint arXiv:2306.06211 (2023).  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=A+Survey+on+Segment+Anything+Model+SAM+Vision+Foundation+Model+Meets+Prompt+Engineering&btnG=) [[Paper]](https://arxiv.org/abs/2306.06211)
5. Yang, Sherry, et al. "Foundation models for decision making: Problems, methods, and opportunities." arXiv preprint arXiv:2303.04129 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Foundation+Models+for+Decision+Making+Problems%2C+Methods%2C+and+Opportunities&btnG=) [[Paper]](https://arxiv.org/abs/2303.04129)
6. Zhang, Xinsong, et al. "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks." arXiv preprint arXiv:2301.05065 (2023).  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Toward+Building+General+Foundation+Models+for+Language%2C+Vision%2C+and+Vision-Language+Understanding+Tasks&btnG=) [[Paper]](https://arxiv.org/abs/2301.05065)
7. Bommasani, Rishi, et al. "On the opportunities and risks of foundation models." arXiv preprint arXiv:2108.07258 (2021). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=On+the+Opportunities+and+Risks+of+Foundation+Models&btnG=) [[Paper]](https://arxiv.org/abs/2108.07258)

## 3. üóÇÔ∏è Taxonomy

In our paper, we divide the textual instructions into four categories.

### 3.1 Language foundation models 
1. Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=BERT+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding&btnG=) [[Paper]](https://arxiv.org/abs/1810.04805)
2. Du, Nan, et al. "Glam: Efficient scaling of language models with mixture-of-experts." International Conference on Machine Learning. PMLR, 2022. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=GLaM+Efficient+Scaling+of+Language+Models+with+Mixture-of-Experts&btnG=) [[Paper]](https://proceedings.mlr.press/v162/du22c.html)
3. Claude 3 [[Website]](https://www.anthropic.com/news/claude-3-family)
   
### 3.2 Vision foundation models 
1. Yuan, Lu, et al. "Florence: A new foundation model for computer vision." arXiv preprint arXiv:2111.11432 (2021). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Florence+A+New+Foundation+Model+for+Computer+Vision&btnG=) [[Paper]](https://arxiv.org/abs/2111.11432)
2. Ramesh, Aditya, et al. "Hierarchical text-conditional image generation with clip latents." arXiv preprint arXiv:2204.06125 (2022). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=DALLE2-Hierarchical+Text-Conditional+Image+Generation+with+CLIP+Latents&btnG=) [[Paper]](https://arxiv.org/abs/2204.06125)
3. Saharia, Chitwan, et al. "Photorealistic text-to-image diffusion models with deep language understanding." Advances in Neural Information Processing Systems 35 (2022): 36479-36494. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=-Imagen-Photorealistic+Text-to-Image+Diffusion+Models+with+Deep+Language+Understanding&btnG=) [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ec795aeadae0b7d230fa35cbaf04c041-Abstract-Conference.html)
4. Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Stable+Diffusion_High-Resolution+Image+Synthesis+with+Latent+Diffusion+Models&btnG=) [[Paper]](https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html)
5. Kang, Minguk, et al. "Scaling up gans for text-to-image synthesis." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=GigaGAN-Scaling+up+GANs+for+Text-to-Image+Synthesis&btnG=) [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html)
6. Cao, Yunkang, et al. "Segment Any Anomaly without Training via Hybrid Prompt Regularization." arXiv preprint arXiv:2305.10724 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=SAA%2B-Segment+Any+Anomaly+without+Training+via+Hybrid+Prompt+Regularization&btnG=) [[Paper]](https://arxiv.org/abs/2305.10724)
7. Zou, Xueyan, et al. "Segment everything everywhere all at once." arXiv preprint arXiv:2304.06718 (2023).  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Segment+everything+everywhere+all+at+once&btnG=) [[Paper]](https://arxiv.org/abs/2304.06718)

### 3.3 Multimodal foundation models 
1. Cherti, Mehdi, et al. "Reproducible scaling laws for contrastive language-image learning." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=OpenCLIP-Reproducible+scaling+laws+for+contrastive+language-image+learning&btnG=) [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.html)
2. Li, Junnan, et al. "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation." International Conference on Machine Learning. PMLR, 2022. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=BLIP_Bootstrapping+Language-Image+Pre-training+for+Unified+Vision-Language+Understanding+and+Generation&btnG=) [[Paper]](https://proceedings.mlr.press/v162/li22n.html)
3. Alayrac, Jean-Baptiste, et al. "Flamingo: a visual language model for few-shot learning." Advances in Neural Information Processing Systems 35 (2022): 23716-23736. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=-Flamingo_a+Visual+Language+Model+for+Few-Shot+Learning&btnG=) [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)
4. Huang, Shaohan, et al. "Language is not all you need: Aligning perception with language models." arXiv preprint arXiv:2302.14045 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0,23&q=KOSMOS-1-Language+Is+Not+All+You+Need+Aligning+Perception+with+Language+Models) [[Paper]](https://arxiv.org/abs/2302.14045)
5. Girdhar, Rohit, et al. "Imagebind: One embedding space to bind them all." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf)
6. Wei, Longhui, et al. "Mvp: Multimodality-guided visual pre-training." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.[[Arxiv]](https://arxiv.org/pdf/2203.05175.pdf)
7. Gemini 1.5. [[Website]](https://deepmind.google/technologies/gemini/#introduction)

### 3.4 Reinforcement learning foundation models 
1. Reed, Scott, et al. "A generalist agent." arXiv preprint arXiv:2205.06175 (2022). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=A+Generalist+Agent&btnG=) [[Paper]](https://arxiv.org/abs/2205.06175)
2. Team, Adaptive Agent, et al. "Human-timescale adaptation in an open-ended task space." arXiv preprint arXiv:2301.07608 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Human-Timescale+Adaptation+in+an+Open-Ended+Task+Space&btnG=) [[Paper]](https://arxiv.org/abs/2301.07608)

## 4. ü§ñ Applications
1. Xu, Yunbi, et al. "Smart breeding driven by big data, artificial intelligence, and integrated genomic-enviromic prediction." Molecular Plant 15.11 (2022): 1664-1695. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Smart+breeding+driven+by+big+data%2C+artificial+intelligence%2C+and+integrated+genomic-enviromic+prediction&btnG=) [[Paper]](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.cell.com/molecular-plant/pdf/S1674-2052(22)00295-7.pdf)
2. Williams, Dominic, Fraser MacFarlane, and Avril Britten. "Leaf Only SAM: A Segment Anything Pipeline for Zero-Shot Automated Leaf Segmentation." arXiv preprint arXiv:2305.09418 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Leaf+Only+SAM+A+Segment+Anything+Pipeline+for+Zero-Shot+Automated+Leaf+Segmentation&btnG=) [[Paper]](https://arxiv.org/abs/2305.09418)
3. Yang, Xiao, et al. "SAM for Poultry Science." arXiv preprint arXiv:2305.10254 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=SAM+for+Poultry+Science&btnG=) [[Paper]](https://arxiv.org/abs/2305.10254)
4. Stella, Francesco, Cosimo Della Santina, and Josie Hughes. "How can LLMs transform the robotic design process?." Nature Machine Intelligence 5.6 (2023): 561-564.  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=How+can+LLMs+transform+the+robotic+design+process%3F&btnG=) [[Paper]](https://www.nature.com/articles/s42256-023-00669-7)
5. Tzachor, Asaf, et al. "Large language models and agricultural extension services." Nature Food 4.11 (2023): 941-948. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Large+language+models+and+agricultural+extension+services&btnG=) [[Paper]](https://www.nature.com/articles/s43016-023-00867-x)
6. Lu, Guoyu, et al. "Agi for agriculture." arXiv preprint arXiv:2304.06136 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Agi+for+agriculture&btnG=) [[Paper]](https://arxiv.org/abs/2304.06136)
7. Yang, Xianjun, et al. "Pllama: An open-source large language model for plant science." arXiv preprint arXiv:2401.01600 (2024). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Pllama%3A+An+open-source+large+language+model+for+plant+science&btnG=) [[Paper]](https://arxiv.org/abs/2401.01600)
8. Shutske, John M. "Harnessing the Power of Large Language Models in Agricultural Safety & Health." Journal of Agricultural Safety and Health (2023): 0. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Harnessing+the+Power+of+Large+Language+Models+in+Agricultural+Safety%26Health&btnG=) [[Paper]](https://elibrary.asabe.org/azdez.asp?JID=3&AID=54486&t=2&v=0&i=0&CID=j0000&downPDF=Y&directPDF=Y)
9. Kuska, Matheus Thomas, Mirwaes Wahabzada, and Stefan Paulus. "Ai-Chatbots for Agriculture-Where Can Large Language Models Provide Substantial Value?." Available at SSRN 4685971. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Ai-Chatbots+for+Agriculture-Where+Can+Large+Language+Models+Provide+Substantial+Value%3F&btnG=) [[Paper]](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4685971)
10. Cao, Yiyi, et al. "Cucumber disease recognition with small samples using image-text-label-based multi-modal language model." Computers and Electronics in Agriculture 211 (2023): 107993. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=Cucumber+disease+recognition+with+small+samples+using+image-text-label-based+multi-modal+language+model&btnG=) [[Paper]](https://www.sciencedirect.com/science/article/pii/S0168169923003812?casa_token=eK3Lkip3GtgAAAAA:HbuFU3XUGseIHIfd9mOc43hSe02exDy_Fubf_fWP9DIs3xDDBFUUI17oCpHZIL19NXOGwO4bNRWm)
11. Stella, Francesco, Cosimo Della Santina, and Josie Hughes. "How can LLMs transform the robotic design process?." Nature Machine Intelligence 5.6 (2023): 561-564. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=How+can+LLMs+transform+the+robotic+design+process%3F&btnG=) [[Paper]](https://www.nature.com/articles/s42256-023-00669-7)
12. Tan, Chenjiao, et al. "On the promises and challenges of multimodal foundation models for geographical, environmental, agricultural, and urban planning applications." arXiv preprint arXiv:2312.17016 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C47&q=On+the+promises+and+challenges+of+multimodal+foundation+models+for+geographical%2C+environmental%2C+agricultural%2C+and+urban+planning+applications&btnG=) [[Paper]](https://arxiv.org/abs/2312.17016)
13. Zhao, Xinyan, Baiyan Chen, Mengxue Ji, Xinyue Wang, Yuhan Yan, Jinming Zhang, Shiyingjie Liu, Muyang Ye, and Chunli Lv. "Implementation of Large Language Models and Agricultural Knowledge Graphs for Efficient Plant Disease Detection." Agriculture 14, no. 8 (2024): 1359.[[Paper]](https://www.mdpi.com/2077-0472/14/8/1359)
14. Fattepur, Bhumika, A. Sakshi, A. Abhishek, and Sneha Varur. "Cultivating Prosperity: A Fusion of IoT Data with Machine Learning and Deep Learning for Precision Crop Recommendations." In 2024 5th International Conference for Emerging Technology (INCET), pp. 1-6. IEEE, 2024. [[Paper]](https://ieeexplore.ieee.org/abstract/document/10593556)
15. Xie, Yiqun, Zhihao Wang, Weiye Chen, Zhili Li, Xiaowei Jia, Yanhua Li, Ruichen Wang, Kangyang Chai, Ruohan Li, and Sergii Skakun. "When are Foundation Models Effective? Understanding the Suitability for Pixel-Level Classification Using Multispectral Imagery." arXiv preprint arXiv:2404.11797 (2024).
16. Qing, Jiajun, Xiaoling Deng, Yubin Lan, and Zhikai Li. "GPT-aided diagnosis on agricultural image based on a new light YOLOPC." Computers and electronics in agriculture 213 (2023): 108168.
