<!-- omit in toc -->
# Awesome Foundation Models in Agricutlture: A Comprehensive Review

[![Awesome](https://awesome.re/badge.svg)](https://github.com/JiajiaLi04/Agriculture-Foundation-Models
) ![](https://img.shields.io/github/stars/JiajiaLi04/Agriculture-Foundation-Models?style=social)

![](https://img.shields.io/github/last-commit/JiajiaLi04/Agriculture-Foundation-Models?color=#00FA9A) ![](https://img.shields.io/badge/PaperNumber-20-blue) ![](https://img.shields.io/badge/PRs-Welcome-red) 

A curated list of awesome **Foundation Models in Agricutlture** papers ğŸ”¥ğŸ”¥ğŸ”¥. 

Currently maintained by <ins>[Jiajia Li](xx) @ MSU</ins>. 



**<font color='red'>Work still in progress</font>**  ğŸš€, **we appreciate any suggestions and contributions** â¤ï¸.

---

<!-- What is instruction learning?
Why instruction learning?
-->

<!-- TODO
## Our scope:
We aim to stay up-to-date with the most innovative developments in the field and gain valuable insights into the future of instruction-learning technology.ğŸ‘€ organize a systematic and comprehensive overview of instructional learning.

1. Stay up-to-date with the most innovative developments in this field.
2. Gain valuable insights into the future of instruction-learning technology.
3. 
-->


<!-- omit in toc -->
## How to contribute?

If you have any suggestions or find any missed papers, feel free to reach out or submit a [pull request](https://github.com/JiajiaLi04/Agriculture-Foundation-Models/pulls):

1. Use following markdown format.

```markdown
*Author 1, Author 2, and Author 3.* **Paper Title.**  <ins>Conference/Journal/Preprint</ins> Year. [[pdf](link)]; [[other resources](link)].
```
<!-- >1. **Paper Title.** *Author 1, Author 2, and Author 3.* Conference/Journal/Preprint Year. [[pdf](link)]. -->

2. If one preprint paper has multiple versions, please use **the earliest submitted year**.
   
3. Display the papers in a **year descending order** (the latest, the first).


<!-- omit in toc -->
## Citation

Find this repository helpful? ğŸ˜Š  

Please consider citing our paper. ğŸ‘‡ğŸ‘‡ğŸ‘‡

*(**Note that the current version of our survey is only a draft, and we are still working on it.**)* ğŸš€

```
coming soon
```

<!-- TODO: add survey citation and link -->

<!-- omit in toc -->
## ğŸ” Table of Contents 

- [1. ğŸ’ğŸ½â€â™€ï¸ Introduction](#1-ï¸-introduction)
- [2. ğŸ“ Surveys and Tutorials](#2--surveys-and-tutorials)
- [3. ğŸ—‚ï¸ Taxonomy](#3-ï¸-taxonomy)
  - [3.1 Language foundation models](#31-language-foundation-models)
  - [3.2 Vision foundation models](#32-vision-foundation-models)
  - [3.3 Multimodal foundation models](#33-multimodal-foundation-models)
  - [3.4 Reinforcement learning foundation models](#34-reinforcement-learning-foundation-models)
- [4. ğŸ¤– Applications](#4--applications)


## 1. ğŸ’ğŸ½â€â™€ï¸ Introduction
Why foundation models instead of traditional deep learning models?
- ğŸ‘‰ **Pre-trained Knowledge.** By training on vast and diverse datasets, FMs possess a form of "general intelligence" that encompasses knowledge of the world, language, vision, and their specific training domains.
- ğŸ‘‰ **Fine-tuning Flexibility.** FMs demonstrate superior performance to be fine-tuned for particular tasks or datasets, saving the computational and temporal investments required to train extensive models from scratch.
- ğŸ‘‰ **Data Efficiency.** FMs harness their foundational knowledge, exhibiting remarkable performance even in the face of limited task-specific data, which is effective for scenarios with data scarcity issues. 

## 2. ğŸ“ Surveys and Tutorials [[Google Scholar]]() [[Paper]]()
1. Moor, Michael, et al. "Foundation models for generalist medical artificial intelligence." Nature 616.7956 (2023): 259-265. [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Foundation+models+for+generalist+medical+artificial+intelligence&btnG=) [[Paper]](https://www.nature.com/articles/s41586-023-05881-4)
2. Mai, Gengchen, et al. "On the opportunities and challenges of foundation models for geospatial artificial intelligence." arXiv preprint arXiv:2304.06798 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=On+the+Opportunities+and+Challenges+of+Foundation+Models+for+Geospatial+Artificial+Intelligence&btnG=) [[Paper]](https://arxiv.org/abs/2304.06798)
3. Stella, Francesco, Cosimo Della Santina, and Josie Hughes. "How can LLMs transform the robotic design process?." Nature Machine Intelligence (2023): 1-4.  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=How+can+LLMs+transform+the+robotic+design+process&btnG=) [[Paper]](https://www.nature.com/articles/s42256-023-00669-7)
4. Zhang, Chaoning, et al. "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering." arXiv preprint arXiv:2306.06211 (2023).  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=A+Survey+on+Segment+Anything+Model+SAM+Vision+Foundation+Model+Meets+Prompt+Engineering&btnG=) [[Paper]](https://arxiv.org/abs/2306.06211)
5. Yang, Sherry, et al. "Foundation models for decision making: Problems, methods, and opportunities." arXiv preprint arXiv:2303.04129 (2023). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Foundation+Models+for+Decision+Making+Problems%2C+Methods%2C+and+Opportunities&btnG=) [[Paper]](https://arxiv.org/abs/2303.04129)
6. Zhang, Xinsong, et al. "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks." arXiv preprint arXiv:2301.05065 (2023).  [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=Toward+Building+General+Foundation+Models+for+Language%2C+Vision%2C+and+Vision-Language+Understanding+Tasks&btnG=) [[Paper]](https://arxiv.org/abs/2301.05065)
7. Bommasani, Rishi, et al. "On the opportunities and risks of foundation models." arXiv preprint arXiv:2108.07258 (2021). [[Google Scholar]](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=On+the+Opportunities+and+Risks+of+Foundation+Models&btnG=) [[Paper]](https://arxiv.org/abs/2108.07258)

## 3. ğŸ—‚ï¸ Taxonomy

In our paper, we divide the textual instructions into four categories.

### 3.1 Language foundation models

### 3.2 Vision foundation models

### 3.3 Multimodal foundation models

### 3.4 Reinforcement learning foundation models

## 4. ğŸ¤– Applications


